{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                             M352: Mathematical Fundations for Machine Learning\n",
    "                                       Prof: Naji YEBARI\n",
    "\n",
    "                            M352: Fondements mathématiques pour l'apprentissage automatique\n",
    "                                                https://d2l.ai/\n",
    "\n",
    "                                                https://www.vertopal.com/en/convert/ipynb-to-pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                      # Preliminaries\n",
    "\n",
    "To prepare for your dive into deep learning, you will need a few survival skills\n",
    "\n",
    "Pour préparer votre plongée dans le deep learning, vous aurez besoin de quelques compétences de survie\n",
    "\n",
    "(i) techniques for storing and manipulating data\n",
    "\n",
    "(i) techniques de stockage et de manipulation des données\n",
    "\n",
    "\n",
    "(ii) libraries for ingesting and preproccessing data from a variety of sources\n",
    "\n",
    "(ii) bibliothèques pour l'ingestion et le prétraitement de données provenant de diverses sources\n",
    "\n",
    "\n",
    "(iii) knowledge of the basic linear algebraic operations that we apply to high-dimensional data elements\n",
    "\n",
    "\n",
    "(iii) connaissance des opérations algébriques linéaires de base que nous appliquons aux éléments de données de grande dimension\n",
    "\n",
    "\n",
    "(iv) just enough calculus to determine which direction to adjust each parameter in order to decrease the loss function\n",
    "\n",
    "\n",
    "(iv) juste assez de calcul pour déterminer dans quelle direction ajuster chaque paramètre afin de diminuer la fonction de perte\n",
    "\n",
    "\n",
    "(v) the ability to automatically compute derivatives so that you can forget much of the calculus you just learned\n",
    "\n",
    "(v) la possibilité de calculer automatiquement les dérivées afin que vous puissiez oublier une grande partie du calcul que vous venez d'apprendre\n",
    "\n",
    "(vi) some basic fluency in probability,  our primary language for reasoning under uncertaity; and\n",
    "\n",
    "\n",
    "(vi) une certaine maîtrise de base des probabilités, notre langage principal pour raisonner en situation d'incertitude ; et\n",
    "\n",
    "\n",
    "(vii) some aptitude for finding answers in the official documentation when you get stuck\n",
    "\n",
    "(vii) une certaine aptitude à trouver des réponses dans la documentation officielle lorsque vous êtes bloqué\n",
    "\n",
    "\n",
    "In short,  this module provides a rapid introduction to the basics that you will need to follow most of the technical content in this Licence\n",
    "\n",
    "\n",
    "En bref, ce module fournit une introduction rapide aux bases dont vous aurez besoin pour suivre la plupart des contenus techniques de cette Licence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                    \n",
    "2.1 Data Manipulation\n",
    "\n",
    "(i) Acquire data : acquirir des données \n",
    "\n",
    "(ii) Process them once they are inside the computer:     les traiter une fois qu'ils sont à l'intérieur du PC \n",
    "\n",
    "\n",
    "2.1.1 Getting Started :      Commencer\n",
    "\n",
    "We import the PyTorch library : import torch\n",
    "\n",
    "\n",
    "A tensor represents array of numerical values.With one axis a tensor is called a vector; with 2 axes, a tensor is called a matrix\n",
    "with $k > 2$ axes, \n",
    "\n",
    "we drop the specialized names and just refer to the object as $k th$ order tensor:\n",
    "\n",
    "nous laissons tomber les noms spécialisés et nous nous référons simplement à l'objet comme un tenseur d'ordre $k$\n",
    "\n",
    "PyTorch provides a variety of functions for creating new tensors prepopulated with values:\n",
    "\n",
    "Pytorch fournit une variété de fonctions pour créer de nouveaux tenseurs pré-remplis avec des valeurs\n",
    "\n",
    "For example ,by invoking  $ arange(n)$ we can create a vector of evenly spaced values, starting at 0 (included) and ending at n (not included);\n",
    "\n",
    "par exemple, en appelant arange(n) nous pouvons créer un vecteur de valeurs réguliérement espacées commençant à 0 inclus et se terminant à n non inclus Par défaut, la taille de l'intervalle est 1. Sauf indication contraire, les nouveaux tenseurs sont stockés dans la mémoire principale et désignés pour le calcul basé sur le CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x=torch.arange(12,dtype=torch.float32)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access a tensor's shape (the length along each axis) by inspecting its shape attribute.Because we are dealing with a vector here, the shape contains just a single element and is identical to the size\n",
    "\n",
    "Nous pouvons accéder à la forme d'un tenseur (la longueur le long de chaque axe) en inspectant son attribut de forme.Parce que nous avons affaire ici à un vecteur, la forme ne contient qu'un seul élément et est identique à la taille"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change the shape of a tensor without altering its size or values, by invoking reshape.For example, we can transform our vector x whose shape is 12, to a matrix X with shape (3,4); This new tensor retains all element but reconfigures them into a matrix. Notice that the elements of our vector are laid out one row at a time and thus x[3]==X[0,3]:\n",
    "\n",
    "Nous pouvons changer la forme d'un tenseur sans modifier sa taille ou ses valeurs, en appelant reshape. Par exemple, nous pouvons transformer notre vecteur x dont la forme est 12, en une matrice X de forme (3,4) ; Ce nouveau tenseur conserve tous les éléments mais les reconfigure dans une matrice. Notez que les éléments de notre vecteur sont disposés une ligne à la fois et donc x[3]==X[0,3] :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=x.reshape(3,4)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note tha specifying every shape component to reshape is redundant. Because we already know our tensor's size, we can work out one component of the shape given the rest. For example, given a tensor of size n and target shape (h,w) we know that w=n/h. To automaticallyinfer one component of the shape, we can place -1 for the shape component that should be inferred automatically. In our case, instead of calling x.reshape(3,4), we could have equivalently called x.reshape(-1,4) or x.reshape(3,-1):\n",
    "\n",
    "Notez que spécifier chaque composante de forme à remodeler est redondant. Parce que nous connaissons déjà la taille de notre tenseur, nous pouvons déterminer une composante de la forme étant donné le reste. Par exemple, étant donné un tenseur de taille n et de forme cible (h,w) nous savons que \n",
    "w=n/h. Pour déduire automatiquement une composante de la forme, nous pouvons placer -1 pour le composant de forme qui doit être déduit automatiquement. Dans notre cas, au lieu d'appeler x.reshape(3,4), nous aurions pu appeler de manière équivalente x.reshape(-1,4) ou \n",
    "x.reshape(3,-1) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape(3,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape(-1,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practitioners, often need to work with tensors initialized to contain all zeros or ones.We can construct a tensor with all elements to zero and shape of (2,3,4) via the zeros function:\n",
    "\n",
    "Les praticiens ont souvent besoin de travailler avec des tenseurs initialisés pour contenir tous les zéros ou les uns. Nous pouvons construire un tenseur avec tous les éléments à zéro et la forme de (2,3,4) via la fonction des zéros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((2,3,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can create a tensor with all ones by invoking ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((2,3,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often wish to sample each element randomly (and independently) from a given probability distribution.For example, the parameters of neural networks are often initialized randomly: The following snippet creates a tensor with elements drawn from a standard Gaussian (normal) distribution with mean 0 and standard deviation 1\n",
    "\n",
    "Nous souhaitons souvent échantillonner chaque élément de manière aléatoire (et indépendante) à partir d'une distribution de probabilité donnée. Par exemple, les paramètres des réseaux de neurones sont souvent initialisés de manière aléatoire : moyenne 0 et écart type 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2561,  1.1990,  1.4799,  2.8214],\n",
       "        [-1.2787,  0.9483,  0.6354, -2.0385],\n",
       "        [ 0.6324, -1.5422,  0.3239,  0.9377]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can construct tensors by supplying the exact values for each element by supplying (possibly nested) Python list(s) containing numerical literals.Here, we construct a matrix with a list of list, where the outermost list corresponds to axis 0, and the inner list to axis 1.\n",
    "\n",
    "Enfin, nous pouvons construire des tenseurs en fournissant les valeurs exactes pour chaque élément en fournissant des listes Python (éventuellement imbriquées) contenant des littéraux numériques. Ici, nous construisons une matrice avec une liste de liste, où la liste la plus externe correspond à l'axe 0, et la liste intérieure à l'axe 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 1, 4, 3],\n",
       "        [1, 2, 3, 4],\n",
       "        [4, 3, 2, 1]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[2,1,4,3],[1,2,3,4],[4,3,2,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.2 Indexing and Slicing :  Indexation et découpage\n",
    "\n",
    "As with Python lists, we can acces tensor elements by indexing (starting with 0). To access an element based on its position relative to the end of the list, we can use negative indexing. Finaly, we can access whole ranges of indices via slicing (e.g., X[start:stop]), where the returned value includes the first index(start) but not last(stop).Finally, when only one index (or slice) is specified for a k th order tensor, it is applied along axis 0. Thus, in the following code,[-1] selects the last row and [1:3] selects the second and third rows.\n",
    "\n",
    "Comme pour les listes Python, nous pouvons accéder aux éléments tenseurs par indexation (en commençant par 0). Pour accéder à un élément en fonction de sa position par rapport à la fin de la liste, on peut utiliser une indexation négative. Enfin, nous pouvons accéder à des plages entières d'indices via le découpage (par exemple, X[start:stop]), où la valeur renvoyée inclut le premier indice (start) mais pas le dernier (stop). Enfin, lorsqu'un seul indice (ou tranche) est spécifié pour un tenseur d'ordre k, il est appliqué le long de l'axe 0. Ainsi, dans le code suivant,[-1] sélectionne la dernière ligne et [1:3] sélectionne les deuxième et troisième lignes.\n",
    "X=tensor([[ 0.,  1.,  2.,  3.],\n",
    "        [ 4.,  5.,  6.,  7.],\n",
    "        [ 8.,  9., 10., 11.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8.,  9., 10., 11.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond reading, we can also write elements of a matrix by specifying indices:\n",
    "\n",
    "Au-delà de la lecture, on peut aussi écrire des éléments d'une matrice en spécifiant des indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5., 17.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1,2]=17\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12., 12., 12., 12.],\n",
       "        [12., 12., 12., 12.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:2,:]=12\n",
    "X"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
