{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                2.3 Linear Algebra : Algèbre linéaire\n",
    "\n",
    "\n",
    "By now, we can load datasets into tensors and manipulate these tensors with basic mathematical operations. To start building sophisticated models, we will also need a few tools from linear algebra. This section offers a gentle introduction to the most essential concepts,\n",
    "starting from scalar arithmetic and ramping up to matrix multiplication.\n",
    "\n",
    "À présent, nous pouvons charger des ensembles de données dans des tenseurs et manipuler ces tenseurs avec des opérations mathématiques de base. Pour commencer à construire des modèles sophistiqués, nous aurons également besoin de quelques outils d'algèbre linéaire. Cette section propose une introduction en douceur aux concepts les plus essentiels,\n",
    "en partant de l'arithmétique scalaire et en passant à la multiplication matricielle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.1 Scalars :Scalaires\n",
    "\n",
    "Most everyday mathematics consists of manipulating numbers one at a time. Formally, we call these values scalars. For example, the temperature in Palo Alto is a balmy 72 degrees Fahrenheit. If you wanted to convert the temperature to Celsius you would evaluate the expression \n",
    "c =5/9(f − 32), setting f to 72. In this equation, the values 5, 9, and 32 are scalars. The variables c and f represent unknown scalars.\n",
    "We denote scalars by ordinary lower-cased letters (e.g., x, y, and z) and the space of all (continuous) real-valued scalars by R. For expedience, we will skip past rigorous definitions of spaces. Just remember that the expression x ∈ R is a formal way to say that x is a real valued scalar. The symbol ∈ (pronounced “in”) denotes membership in a set. For example, x, y ∈ {0, 1} indicates that x and y are variables that can only take values 0 or 1. Scalars are implemented as tensors that contain only one element. Below, we assign two scalars and perform the familiar addition, multiplication, division, and exponentiation operations\n",
    "\n",
    "La plupart des mathématiques courantes consistent à manipuler des nombres un par un. Formellement, nous appelons ces valeurs des scalaires. Par exemple, la température à Palo Alto est de 72 degrés Fahrenheit. Si vous vouliez convertir la température en degrés Celsius, vous évalueriez l'expression c =5/9(f − 32), en fixant f à 72. Dans cette équation, les valeurs 5, 9 et 32 ​​sont des scalaires. Les variables c et f représentent des scalaires inconnus.\n",
    "Nous désignons les scalaires par des lettres minuscules ordinaires (par exemple, x, y et z) et l'espace de tous les scalaires à valeur réelle (continus) par R. Par commodité, nous sauterons les définitions rigoureuses des espaces. Rappelez-vous simplement que l'expression x ∈ R est une façon formelle de dire que x est un scalaire à valeur réelle. Le symbole ∈ (prononcé « in ») dénote l'appartenance à un ensemble. Par exemple, x, y ∈ {0, 1} indique que x et y sont des variables qui ne peuvent prendre que les valeurs 0 ou 1. Les scalaires sont implémentés comme des tenseurs qui ne contiennent qu'un seul élément. Ci-dessous, nous attribuons deux scalaires et effectuons les opérations habituelles d'addition, de multiplication, de division et d'exponentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(3.0)\n",
    "y = torch.tensor(2.0)\n",
    "x + y, x * y, x / y, x**y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.2 Vectors :  Vecteurs\n",
    "For our purposes, you can think of vectors as fixed-length arrays of scalars. As with their code counterparts, we call these values the elements of the vector (synonyms include entries and components). When vectors represent examples from real-world datasets, their values hold some real-world significance. For example, if we were training a model to predict the risk of a loan defaulting, we might associate each applicant with a vector whose components correspond to quantities like their income, length of employment, or number of previous defaults. If we were studying heart attack risk, each vector might represent a patient and its components might correspond to their most recent vital signs, cholesterol levels, minutes of exercise per day, etc. We denote vectors by bold lowercase letters, (e.g., x, y, and z). Vectors are implemented as 1st-order tensors. In general, such tensors can have arbitrary lengths, subject to memory limitations. Caution: in Python, like in most programming languages, vector indices start at 0, also known as zero-based indexing, whereas in linear algebra subscripts begin at 1 (one-based indexing).\n",
    "\n",
    "Pour nos besoins, vous pouvez considérer les vecteurs comme des tableaux de scalaires de longueur fixe. Comme pour leurs homologues de code, nous appelons ces valeurs les éléments du vecteur (les synonymes incluent les entrées et les composants). Lorsque les vecteurs représentent des exemples d'ensembles de données du monde réel, leurs valeurs ont une certaine signification dans le monde réel. Par exemple, si nous entraînons un modèle pour prédire le risque de défaut de paiement d'un prêt, nous pourrions associer chaque demandeur à un vecteur dont les composants correspondent à des quantités telles que son revenu, sa durée d'emploi ou le nombre de défauts de paiement antérieurs. Si nous étudiions le risque de crise cardiaque, chaque vecteur pourrait représenter un patient et ses composants pourraient correspondre à ses signes vitaux les plus récents, son taux de cholestérol, ses minutes d'exercice par jour, etc. Nous désignons les vecteurs par des lettres minuscules en gras (par exemple, x, y et z). Les vecteurs sont implémentés sous forme de tenseurs du 1er ordre. En général, ces tenseurs peuvent avoir des longueurs arbitraires, sous réserve de limitations de mémoire. Attention : en Python, comme dans la plupart des langages de programmation, les indices vectoriels commencent à 0, également appelé indexation à base zéro, alors qu'en algèbre linéaire les indices commencent à 1 (indexation à base un).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=\n",
      " tensor([0, 1, 2])\n",
      "x.T=\n",
      " tensor([[0],\n",
      "        [1],\n",
      "        [2]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False],\n",
       "        [False,  True, False],\n",
       "        [False, False,  True]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(3)\n",
    "x,torch.arange(3).T,torch.arange(3).T.reshape(-1,1)\n",
    "x,x==torch.arange(3).T,x.T.reshape(-1,1),\n",
    "x==torch.arange(3).T.reshape(-1,1)\n",
    "print(\"x=\\n\",x)\n",
    "print(\"x.T=\\n\",x.T.reshape(-1,1))\n",
    "x==torch.arange(3).T.reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x==torch.arange(3).T.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [0, 1, 2],\n",
       "        [0, 1, 2]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa=torch.tensor([[0, 1, 2],[0, 1, 2],[0, 1, 2]])\n",
    "aa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can refer to an element of a vector by using a subscript. For example, $x_2$ denotes the\n",
    "second element of x. Since  $x_2$ is a scalar, we do not bold it. By default, we visualize vectors\n",
    "by stacking their elements vertically.\n",
    "$$\n",
    "x=\\left( \\begin{array}{c}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_i \\\\\n",
    "\\vdots \\\\\n",
    "x_n\n",
    "\\end{array} \\right) \\quad \\quad (2.3.1)\n",
    "$$\n",
    "\n",
    "Here $x_1, . . ., x_n $ are elements of the vector. Later on, we will distinguish between such column\n",
    "vectors and row vectors whose elements are stacked horizontally. Recall that we access a\n",
    "tensor’s elements via indexing.\n",
    "\n",
    "Nous pouvons faire référence à un élément d'un vecteur en utilisant un indice. Par exemple, $x_2$ désigne le deuxième élément de x. Puisque  $x_2$ est un scalaire, nous ne le mettons pas en gras. Par défaut, nous visualisons des vecteurs\n",
    "en empilant leurs éléments verticalement.\n",
    "$$\n",
    "x=\\left( \\begin{array}{c}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_i \\\\\n",
    "\\vdots \\\\\n",
    "x_n\n",
    "\\end{array} \\right) \\quad \\quad (2.3.1)\n",
    "$$\n",
    "ici $x_1, . . ., x_n$  sont des éléments du vecteur. Plus tard, nous distinguerons une telle colonne vecteurs et vecteurs ligne dont les éléments sont empilés horizontalement. Rappelons que nous accédons à un éléments du tenseur via l'indexation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2), tensor(1))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.arange(3)\n",
    "x[2],x[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To indicate that a vector contains n elements, we write $x ∈ \\mathbb{R }^n$ . Formally, we call n the dimensionality of the vector. In code, this corresponds to the tensor’s length, accessible via Python’s built-in len function.\n",
    "\n",
    "Pour indiquer qu'un vecteur contient n éléments, on écrit $x ∈ \\mathbb{R }^n$ . Formellement, nous appelons n la dimensionnalité du vecteur. Dans le code, cela correspond à la longueur du tenseur, accessible via la fonction len intégrée de Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, torch.Size([3]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x),x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also access the length via the shape attribute. The shape is a tuple that indicates a tensor’s length along each axis. Tensors with just one axis have shapes with just one element.\n",
    "\n",
    "Nous pouvons également accéder à la longueur via l'attribut shape. La forme est un tuple qui indique la longueur d'un tenseur le long de chaque axe. Les tenseurs avec un seul axe ont des formes avec un seul élément.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oftentimes, the word “dimension” gets overloaded to mean both the number of axes and the length along a particular axis. To avoid this confusion, we use order to refer to the number of axes and dimensionality exclusively to refer to the number of components.\n",
    "\n",
    "Souvent, le mot \"dimension\" est surchargé pour signifier à la fois le nombre d'axes et la longueur le long d'un axe particulier. Pour éviter cette confusion, nous utilisons l'ordre pour désigner le nombre d'axes et la dimensionnalité exclusivement pour désigner le nombre de composants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.3 Matrices : Matrices\n",
    "\n",
    "Just as scalars are 0 th-order tensors and vectors are 1st-order tensors, matrices are $2^nd$-order tensors. We denote matrices by bold capital letters (e.g., X, Y, and Z), and represent them in code by tensors with two axes. The expression $A ∈ \\mathbb{R}^ {m\\times n}$ indicates that a matrix A contains m × n real-valued scalars, arranged as m rows and n columns. When m = n, we say that a matrix is square. Visually, we can illustrate any matrix as a table. To refer to an individual element, we subscript both the row and column indices, e.g., ai j is the value that belongs to A’s $i^\n",
    "th$  row and $j^th$  column:\n",
    "$$\n",
    "A=\\begin{pmatrix}\n",
    "a_{11} &~ a_{12} & . . . &a_{1n} \\\\\n",
    "a_{21} & a_{22} & . . . &a_{2n} \\\\\n",
    ". & . & . & . \\\\                       \n",
    ". & . & . & . \\\\\n",
    ". & . & . & . \\\\\n",
    " a_{m1} &  a_{m2} & . . . &a_{mn}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "In code, we represent a matrix $A ∈ \\mathbb{R}^ {m\\times n}$ by a 2nd-order tensor with shape (m, n). We canconvert any appropriately sized m×n tensor into an m×n matrix by passing the desired shape\n",
    "to reshape:\n",
    "\n",
    "Tout comme les scalaires sont des tenseurs d'ordre 0 et les vecteurs sont des tenseurs d'ordre 1, les matrices sont des tenseurs d'ordre 2. Nous désignons les matrices par des lettres majuscules en gras (par exemple, X, Y et Z) et les représentons dans le code par des tenseurs à deux axes. L'expression A ∈ R m×n indique qu'une matrice A contient\n",
    "m × n scalaires à valeurs réelles, disposés en m lignes et n colonnes. Lorsque m = n, on dit qu'une matrice est carrée. Visuellement, nous pouvons illustrer n'importe quelle matrice sous forme de tableau. Pour faire référence à un élément individuel, nous indicons à la fois les indices de ligne et de colonne, par exemple, ai j est la valeur qui appartient aux i de A\n",
    "ème ligne et jème colonne :\n",
    "$$\n",
    "A=\\begin{pmatrix}\n",
    "a_{11} &~ a_{12} & . . . &a_{1n} \\\\\n",
    "a_{21} & a_{22} & . . . &a_{2n} \\\\\n",
    ". & . & . & . \\\\                       \n",
    ". & . & . & . \\\\\n",
    ". & . & . & . \\\\\n",
    " a_{m1} &  a_{m2} & . . . &a_{mn}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "En code, on représente une matrice $A ∈ \\mathbb{R}^ {m\\times n}$ par un tenseur d'ordre 2 de forme (m, n). Nous pouvons convertir tout tenseur m × n de taille appropriée en une matrice m × n en passant la forme souhaitée\n",
    "remodeler :\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(6).reshape(3, 2)\n",
    "A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B=torch.arange(6).reshape(2, 3)\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we want to flip the axes. When we exchange a matrix’s rows and columns, the result is called its transpose. Formally, we signify a matrix A’s transpose by $A^{⊤}$ and if $B = A^{⊤}$, then $b_{ij} = a_{ji}$ for all i and j. Thus, the transpose of an m × n matrix is an n × m matrix:\n",
    "\n",
    "Parfois, nous voulons inverser les axes. Lorsque nous échangeons les lignes et les colonnes d'une matrice, le résultat s'appelle sa transposition. Formellement, on signifie la transposée d'une matrice A par $A^⊤$ et si $B = A^⊤$, alors \n",
    "$b_{ij} = a_{ji}$ pour tout i et j. Ainsi, la transposée d'une matrice m × n est une matrice n × m\n",
    "$$\n",
    "A=\\begin{pmatrix}\n",
    "a_{11} &~ a_{21} & . . . &a_{m1} \\\\\n",
    "a_{12} & a_{22} & . . . &a_{m2} \\\\\n",
    ". & . & . & . \\\\                       \n",
    ". & . & . & . \\\\\n",
    ". & . & . & . \\\\\n",
    " a_{1n} &  a_{2n} & . . . &a_{mn}\n",
    "\\end{pmatrix} \\quad \\quad (2.3.3)\n",
    "$$\n",
    "In code, we can access any matrix’s transpose as follows:\n",
    "Dans le code, nous pouvons accéder à la transposé de n'importe quelle matrice comme suit :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 2, 4],\n",
       "        [1, 3, 5]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Symmetric matrices are the subset of square matrices that are equal to their own transposes: $A = A^⊤$. The following matrix is symmetric:\n",
    "\n",
    "Les matrices symétriques sont le sous-ensemble de matrices carrées qui sont égales à leurs propres transposées : $A = A^⊤$. La matrice suivante est symétrique :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A=\n",
      " tensor([[1, 2, 3],\n",
      "        [2, 0, 4],\n",
      "        [3, 4, 5]])\n",
      "A.T=\n",
      " tensor([[1, 2, 3],\n",
      "        [2, 0, 4],\n",
      "        [3, 4, 5]])\n",
      "A == A.T=\n",
      " tensor([[True, True, True],\n",
      "        [True, True, True],\n",
      "        [True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\n",
    "A == A.T\n",
    "print(\"A=\\n\",A )\n",
    "print(\"A.T=\\n\",A.T )\n",
    "print(\"A == A.T=\\n\",A == A.T )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [2, 0, 4],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B=A.T\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1, 10,  3],\n",
       "        [10,  0,  4],\n",
       "        [ 3,  4,  5]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[0][1]=10 \n",
    "A[1][0]=10\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False,  True],\n",
       "        [False,  True,  True],\n",
       "        [ True,  True,  True]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A==B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A.T=\n",
      " tensor([[ 1,  2,  3],\n",
      "        [10,  0,  4],\n",
      "        [ 3,  4,  5]])\n"
     ]
    }
   ],
   "source": [
    "A.T\n",
    "print(\"A.T=\\n\",A.T )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrices are useful for representing datasets. Typically, rows correspond to individual records and columns correspond to distinct attributes.\n",
    "\n",
    "Les matrices sont utiles pour représenter des ensembles de données. Généralement, les lignes correspondent à des enregistrements individuels et les colonnes correspondent à des attributs distincts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.4 Tensors\n",
    "While you can go far in your machine learning journey with only scalars, vectors, and matrices, eventually you may need to work with higher-order tensors. Tensors give us a generic way to describe extensions to n th-order arrays. We call software objects of the tensor class\n",
    "“tensors” precisely because they too can have arbitrary numbers of axes. While it may be confusing to use the word tensor for both the mathematical object and its realization in code,\n",
    "\n",
    "2.3.4 Tenseurs\n",
    "Bien que vous puissiez aller loin dans votre parcours d'apprentissage automatique avec uniquement des scalaires, des vecteurs et des matrices, vous devrez peut-être éventuellement travailler avec des tenseurs d'ordre supérieur. Les tenseurs nous donnent un moyen générique de décrire les extensions aux tableaux d'ordre n. Nous appelons les objets logiciels de la classe tenseur\n",
    "« tenseurs » précisément parce qu'eux aussi peuvent avoir un nombre arbitraire d'axes. Bien qu'il puisse être déroutant d'utiliser le mot tenseur à la fois pour l'objet mathématique et sa réalisation dans le code,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our meaning should usually be clear from context. We denote general tensors by capital letters with a special font face (e.g., X, Y, and Z) and their indexing mechanism (e.g., xi jk and [X]1,2i−1,3) follows naturally from that of matrices.\n",
    "Tensors will become more important when we start working with images. Each image arrives as a 3rd-order tensor with axes corresponding to the height, width, and channel. At each spatial location, the intensities of each color (red, green, and blue) are stacked along the channel.\n",
    "Moreover a collection of images is represented in code by a 4th-order tensor, where distinct images are indexed along the first axis. Higher-order tensors are constructed analogously to vectors and matrices, by growing the number of shape components.\n",
    "\n",
    "notre sens doit généralement être clair à partir du contexte. Nous désignons les tenseurs généraux par des lettres majuscules avec une police spéciale (par exemple, X, Y et Z) et leur mécanisme d'indexation (par exemple, xi jk et [X]1,2i−1,3) découle naturellement de celui des matrices.\n",
    "Les tenseurs deviendront plus importants lorsque nous commencerons à travailler avec des images. Chaque image arrive sous la forme d'un tenseur de 3ème ordre avec des axes correspondant à la hauteur, la largeur et le canal. À chaque emplacement spatial, les intensités de chaque couleur (rouge, vert et bleu) sont empilées le long du canal.\n",
    "De plus, une collection d'images est représentée en code par un tenseur d'ordre 4, où des images distinctes sont indexées le long du premier axe. Les tenseurs d'ordre supérieur sont construits de manière analogue aux vecteurs et aux matrices, en augmentant le nombre de composants de forme.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(24).reshape(2, 3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.5 Basic Properties of Tensor Arithmetic\n",
    "Scalars, vectors, matrices, and higher-order tensors all have some handy properties. For example, elementwise operations produce outputs that have the same shape as their operand\n",
    "\n",
    "2.3.5 Propriétés de base de l'arithmétique tensorielle\n",
    "Les scalaires, les vecteurs, les matrices et les tenseurs d'ordre supérieur ont tous des propriétés pratiques. Par exemple, les opérations élément par élément produisent des sorties qui ont la même forme que leur opérande\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 2.],\n",
       "         [3., 4., 5.]]),\n",
       " tensor([[0., 1., 2.],\n",
       "         [3., 4., 5.]]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(6, dtype=torch.float32).reshape(2, 3)\n",
    "B = A.clone() # Assign a copy of A to B by allocating new memory:\n",
    "#Attribuer une copie de A à B en allouant une nouvelle mémoire\n",
    "A,B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  2.,  4.],\n",
       "        [ 6.,  8., 10.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A+B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elementwise product of two matrices is called their Hadamard product (denoted ⊙). Below, we spell out the entries of the Hadamard product of two matrices A, B ∈ Rm×n :\n",
    "\n",
    "Le produit élément par élément de deux matrices est appelé leur produit de Hadamard (noté ⊙). Ci-dessous, nous épelons les entrées du produit de Hadamard de deux matrices A, B $\\in  \\mathbb{R}^{m\\times n}$ :\n",
    "\n",
    "$$\n",
    "A⊙B=\\begin{pmatrix}\n",
    "a_{11}b_{11} &~ a_{12}b_{12} & . . . &a_{1n} b_{1n}\\\\\n",
    "a_{21}b_{21}  & a_{22}b_{22} & . . . &a_{2n}b_{2n} \\\\\n",
    ". & . & . & . \\\\                       \n",
    ". & . & . & . \\\\\n",
    ". & . & . & . \\\\\n",
    " a_{m1}b_{m1} &  a_{m2} b_{m2}& . . . &a_{mn}b_{mn}\n",
    "\\end{pmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  4.],\n",
       "        [ 9., 16., 25.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A * B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding or multiplying a scalar and a tensor produces a result with the same shape as the original tensor. Here, each element of the tensor is added to (or multiplied by) the scalar\n",
    "\n",
    "L'ajout ou la multiplication d'un scalaire et d'un tenseur produit un résultat ayant la même forme que le tenseur d'origine. Ici, chaque élément du tenseur est ajouté (ou multiplié par) le scalaire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2,  3,  4,  5],\n",
       "         [ 6,  7,  8,  9],\n",
       "         [10, 11, 12, 13]],\n",
       "\n",
       "        [[14, 15, 16, 17],\n",
       "         [18, 19, 20, 21],\n",
       "         [22, 23, 24, 25]]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 2\n",
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "a + X \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a *X =\n",
      " tensor([[[ 0,  2,  4,  6],\n",
      "         [ 8, 10, 12, 14],\n",
      "         [16, 18, 20, 22]],\n",
      "\n",
      "        [[24, 26, 28, 30],\n",
      "         [32, 34, 36, 38],\n",
      "         [40, 42, 44, 46]]])\n",
      "(a * X).shape =\n",
      " torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "(a * X).shape\n",
    "print(\"a *X =\\n\",a * X )\n",
    "print(\"(a * X).shape =\\n\",(a * X).shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrêt le 9-12-2024: stopped in this date "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.6 Reduction\n",
    "\n",
    "Often, we wish to calculate the sum of a tensor’s elements. To express the sum of the elements in a vector x of length n, we write \n",
    " $$ \\sum _{i=1}^{n} {x_{i}} $$ \n",
    " There’s a simple function for it:\n",
    "\n",
    "2.3.6 Réduction\n",
    "\n",
    "Souvent, on souhaite calculer la somme des éléments d'un tenseur. Pour exprimer la somme des éléments dans un vecteur x de longueur n, on écrit\n",
    " $$ \\sum _{i=1}^{n} {x_{i}} $$\n",
    " Il y a une fonction simple pour cela:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2.]), tensor(3.))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(3, dtype=torch.float32)\n",
    "x, x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To express sums over the elements of tensors of arbitrary shape, we simply sum over all of its axes. For example, the sum of the elements of an m × n matrix A could be written\n",
    "\n",
    "Pour exprimer des sommes sur les éléments de tenseurs de forme arbitraire, nous additionnons simplement sur tous ses axes. Par exemple, la somme des éléments d'une matrice m × n A pourrait s'écrire\n",
    "\n",
    "$$ \\sum _{i=1}^{m} \\sum _{j=1}^{n}  {a_{ij}} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 2.],\n",
       "         [3., 4., 5.]]),\n",
       " torch.Size([2, 3]),\n",
       " tensor(15.))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(6, dtype=torch.float32).reshape(2, 3)\n",
    "A,A.shape, A.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, invoking the sum function reduces a tensor along all of its axes, eventually producing a scalar. Our libraries also allow us to specify the axes along which the tensor should be reduced. To sum over all elements along the rows (axis 0), we specify axis=0 in sum.\n",
    "Since the input matrix reduces along axis 0 to generate the output vector, this axis is missing from the shape of the output.\n",
    "\n",
    "\n",
    "Par défaut, l'appel de la fonction somme réduit un tenseur le long de tous ses axes, produisant éventuellement un scalaire. Nos bibliothèques nous permettent également de spécifier les axes selon lesquels le tenseur doit être réduit. Pour additionner tous les éléments le long des lignes (axe 0), nous spécifions axis=0 in sum.\n",
    "Étant donné que la matrice d'entrée se réduit le long de l'axe 0 pour générer le vecteur de sortie, cet axe est absent de la forme de la sortie.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 2.],\n",
       "         [3., 4., 5.]]),\n",
       " torch.Size([2, 3]),\n",
       " torch.Size([3]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(6, dtype=torch.float32).reshape(2, 3)\n",
    "A,A.shape, A.sum(axis=0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying axis=1 will reduce the column dimension (axis 1) by summing up elements of all the columns\n",
    "\n",
    "Spécifier axe = 1 réduira la dimension de la colonne (axe 1) en additionnant les éléments de toutes les colonnes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 2.],\n",
       "         [3., 4., 5.]]),\n",
       " torch.Size([2, 3]),\n",
       " torch.Size([2]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(6, dtype=torch.float32).reshape(2, 3)\n",
    "A,A.shape, A.sum(axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3., 12.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 5., 7.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing a matrix along both rows and columns via summation is equivalent to summing up all the elements of the matrix.\n",
    "\n",
    "Réduire une matrice le long des lignes et des colonnes via la sommation équivaut à sommer tous les éléments de la matrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum(axis=[0, 1]) == A.sum() # Same as A.sum():Identique à A.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A related quantity is the mean, also called the average. We calculate the mean by dividing the sum by the total number of elements. Because computing the mean is so common, it gets a dedicated library function that works analogously to sum.\n",
    "\n",
    "Une quantité liée est la moyenne, également appelée moyenne. Nous calculons la moyenne en divisant la somme par le nombre total d'éléments. Parce que le calcul de la moyenne est si courant, il obtient une fonction de bibliothèque dédiée qui fonctionne de manière analogue à la somme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 2.],\n",
       "         [3., 4., 5.]]),\n",
       " 6,\n",
       " tensor(15.),\n",
       " tensor(2.5000),\n",
       " tensor(2.5000))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(6, dtype=torch.float32).reshape(2, 3)\n",
    "A,A.numel(),A.sum(), A.sum() / A.numel(),A.mean(),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 2.],\n",
       "         [3., 4., 5.]]),\n",
       " 2,\n",
       " tensor([3., 5., 7.]),\n",
       " tensor([1.5000, 2.5000, 3.5000]),\n",
       " tensor([1.5000, 2.5000, 3.5000]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(6, dtype=torch.float32).reshape(2, 3)\n",
    "A,A.shape[0],A.sum(axis=0), A.sum(axis=0) / A.shape[0],A.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.7 Non-Reduction Sum: \n",
    "Sometimes it can be useful to keep the number of axes unchanged when invoking the function for calculating the sum or mean. This matters when we want to use the broadcast mechanism.\n",
    "\n",
    "2.3.7 Somme sans réduction\n",
    "\n",
    "Parfois, il peut être utile de conserver le nombre d'axes inchangé lors de l'appel de la fonction pour calculer la somme ou la moyenne. C'est important quand on veut utiliser le mécanisme de diffusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 2.],\n",
       "         [3., 4., 5.]]),\n",
       " tensor([[ 3.],\n",
       "         [12.]]),\n",
       " torch.Size([2, 1]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(6, dtype=torch.float32).reshape(2, 3)\n",
    "sum_A = A.sum(axis=1, keepdims=True)\n",
    "A,sum_A, sum_A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, since sum_A keeps its two axes after summing each row, we can divide A by sum_A with broadcasting to create a matrix where each row sums up to 1.\n",
    "\n",
    "Par exemple, puisque sum_A conserve ses deux axes après avoir additionné chaque ligne, nous pouvons diviser A par sum_A avec diffusion pour créer une matrice où chaque ligne totalise 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 2.],\n",
       "         [3., 4., 5.]]),\n",
       " tensor([[ 3.],\n",
       "         [12.]]),\n",
       " tensor([[0.0000, 0.3333, 0.6667],\n",
       "         [0.2500, 0.3333, 0.4167]]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(6, dtype=torch.float32).reshape(2, 3)\n",
    "A,sum_A,A / sum_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to calculate the cumulative sum of elements of A along some axis, say axis=0 (row by row), we can call the cumsum function. By design, this function does not reduce the input tensor along any axis.\n",
    "\n",
    "Si nous voulons calculer la somme cumulée des éléments de A le long d'un axe, disons axe = 0 (ligne par ligne), nous pouvons appeler la fonction cumsum. De par sa conception, cette fonction ne réduit le tenseur d'entrée le long d'aucun axe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 2.],\n",
       "         [3., 4., 5.]]),\n",
       " tensor([[0., 1., 2.],\n",
       "         [3., 5., 7.]]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(6, dtype=torch.float32).reshape(2, 3)\n",
    "A,A.cumsum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.8 Dot Products\n",
    "\n",
    "So far, we have only performed elementwise operations, sums, and averages. And if this was all we could do, linear algebra would not deserve its own section. Fortunately, this is where things get more interesting. One of the most fundamental operations is the dot product. Given\n",
    "two vectors $x,y \\in \\mathbb{R}^{d}$  their dot product $ x^{T}y=\\langle x, y \\rangle $ is  a sum over the products of the elements at the same position:\n",
    "$$x^{T}y= \\sum _{i=1}^{d} {x_{i}y_{i}}$$\n",
    ".\n",
    "2.3.8 Produits scalaires\n",
    "\n",
    "Jusqu'à présent, nous n'avons effectué que des opérations élémentaires, des sommes et des moyennes. Et si c'était tout ce que nous pouvions faire, l'algèbre linéaire ne mériterait pas sa propre section. Heureusement, c'est là que les choses deviennent plus intéressantes. L'une des opérations les plus fondamentales est le produit scalaire. Donné\n",
    "deux vecteurs $x,y \\in \\mathbb{R}^{d}$ leur produit scalaire $ x^{T}y=\\langle x, y \\rangle $ est une somme sur les produits des coordonnées des vecteurs x et y:\n",
    "$$x^{T}y= \\sum _{i=1}^{d} {x_{i}y_{i}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2.]), tensor([1., 1., 1.]), tensor(3.))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(3, dtype=torch.float32)\n",
    "y = torch.ones(3, dtype = torch.float32)\n",
    "x, y, torch.dot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalently, we can calculate the dot product of two vectors by performing an elementwise multiplication followed by a sum:\n",
    "\n",
    "De manière équivalente, nous pouvons calculer le produit scalaire de deux vecteurs en effectuant une multiplication élément par élément suivie d'une somme :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(x*y) # x=torch.arange(3, dtype=torch.float32)=(tensor([0., 1., 2.]);\n",
    "#y=torch.ones(3, dtype = torch.float32)=tensor([1., 1., 1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dot products are useful in a wide range of contexts. For example, given some set of values, denoted by a vector  $x \\in \\mathbb{R}^{n}$\n",
    "and a set of weights denoted by $w \\in \\mathbb{R}^{n}$, the weighted sum of the values in x according to the weights w could be expressed as the dot product $ x^{T}w=\\langle x, w \\rangle $. When the weights are non-negative and sum to one, i.e., \n",
    "$$ \\sum _{i=1}^{n} {w_{i}} = 1 ,$$\n",
    " the dot product expresses a weighted average. After normalizing two vectors to have unit length, the dot products express the cosine of the angle between them. Later in this section, we will formally introduce this notion of \n",
    " \n",
    " Les produits scalaires sont utiles dans un large éventail de contextes. Par exemple, étant donné un ensemble de valeurs, désigné par un vecteur $x \\in \\mathbb{R}^{n}$\n",
    "et un ensemble de poids noté $w \\in \\mathbb{R}^{n}$, la somme pondérée des valeurs de x selon les poids w pourrait être exprimée comme le produit scalaire $ x^{T}w= \\langle x, w \\rangle $. Lorsque les poids ne sont pas négatifs et que leur somme est égale à un, c'est-à-dire\n",
    "$$ \\sum _{i=1}^{n} {w_{i}} = 1 ,$$\n",
    " le produit scalaire exprime une moyenne pondérée. Après avoir normalisé deux vecteurs pour avoir une longueur unitaire, les produits scalaires expriment le cosinus de l'angle entre eux. Plus loin dans cette section, nous introduirons formellement cette notion de longueur\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.9 Matrix-Vector Products\n",
    "\n",
    "Now that we know how to calculate dot products, we can begin to understand the product between an m × n matrix A and an n-dimensional vector x. To start off, we visualize our matrix in terms of its row vectors where each $a_{i}^{T} \\in \\mathbb{R}^{n}$ is a row vector representing the $i^{th}$ row of the matrix A.\n",
    "$$\n",
    "A=\\left( \\begin{array}{c}\n",
    "a_{1}^{T} \\\\\n",
    "a_{2}^{T} \\\\\n",
    "\\vdots \\\\\n",
    "a_{i}^{T} \\\\\n",
    "\\vdots \\\\\n",
    "a_{m}^{T}\n",
    "\\end{array} \\right) \\quad \\quad (2.3.5)\n",
    "$$\n",
    "\n",
    "2.3.9 Produits matrice-vecteur\n",
    "\n",
    "Maintenant que nous savons comment calculer les produits scalaires, nous pouvons commencer à comprendre le produit entre une matrice A d'ordre  m × n et un vecteur x à n dimensions. Pour commencer, nous visualisons notre matrice en termes de ses vecteurs lignes où chaque \n",
    "$a_{i}^{T} \\in \\mathbb{R}^{n}$ est un vecteur ligne représentant le $i^{th}$ ligne de la matrice A.\n",
    "$$\n",
    "A=\\left( \\begin{array}{c}\n",
    "a_{1}^{T} \\\\\n",
    "a_{2}^{T} \\\\\n",
    "\\vdots \\\\\n",
    "a_{i}^{T} \\\\\n",
    "\\vdots \\\\\n",
    "a_{m}^{T}\n",
    "\\end{array} \\right) \\quad \\quad (2.3.5)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix-vector product Ax is simply a column vector of length m, whose $i^{th}$  element is the dot product $a_{i}^{T} x $:\n",
    "Le produit matrice-vecteur Ax est simplement un vecteur colonne de longueur m, dont l'élément $i^{th}$ est le produit scalaire $a_{i}^{T} x $:\n",
    "$$\n",
    "Ax=\\left( \\begin{array}{c}\n",
    "a_{1}^{T} \\\\\n",
    "a_{2}^{T} \\\\\n",
    "\\vdots \\\\\n",
    "a_{i}^{T} \\\\\n",
    "\\vdots \\\\\n",
    "a_{m}^{T}\n",
    "\\end{array} \\right) x=  \\left( \\begin{array}{c}\n",
    "a_{1}^{T}x \\\\\n",
    "a_{2}^{T}x \\\\\n",
    "\\vdots \\\\\n",
    "a_{i}^{T}x \\\\\n",
    "\\vdots \\\\\n",
    "a_{m}^{T}x\n",
    "\\end{array} \\right)\\quad \\quad (2.3.6)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of multiplication with a matrix $A \\in \\mathbb{R}^{m \\times n}$ as a transformation that projects\n",
    "vectors from $ \\mathbb{R}^{n}$ to $ \\mathbb{R}^{m}$. These transformations are remarkably useful. For example, we can represent rotations as multiplications by certain square matrices. Matrix-vector products also describe the key calculation involved in computing the outputs of each layer in a neural network given the outputs from the previous layer. To express a matrix-vector product in code, we use the mv function. Note that the column dimension of A (its length along axis 1) must be the same as the dimension of x (its length). PyTorch has a convenience operator @ that can execute both matrix-vector and matrix-matrix products (depending on its arguments). Thus we can write A@x.\n",
    "\n",
    "Nous pouvons considérer la multiplication avec une matrice $A \\in \\mathbb{R}^{m \\times n}$ comme une transformation qui projette\n",
    "vecteurs de $ \\mathbb{R}^{n}$ à $ \\mathbb{R}^{m}$. Ces transformations sont remarquablement utiles. Par exemple, nous pouvons représenter les rotations comme des multiplications par certaines matrices carrées. Les produits matrice-vecteur décrivent également le calcul clé impliqué dans le calcul des sorties de chaque couche dans un réseau de neurones compte tenu des sorties de la couche précédente. Pour exprimer un produit matrice-vecteur dans le code, nous utilisons la fonction mv. Notez que la dimension de colonne de A (sa longueur le long de l'axe 1) doit être la même que la dimension de x (sa longueur). PyTorch a un opérateur de commodité @ qui peut exécuter à la fois des produits matrice-vecteur et matrice-matrice (selon ses arguments). Ainsi on peut écrire A@x.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 2.],\n",
       "         [3., 4., 5.]]),\n",
       " tensor([0., 1., 2.]),\n",
       " torch.Size([2, 3]),\n",
       " torch.Size([3]),\n",
       " tensor([ 5., 14.]),\n",
       " tensor([ 5., 14.]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(6, dtype=torch.float32).reshape(2, 3)#A=(tensor([[0., 1., 2.],[3., 4., 5.]])\n",
    "x = torch.arange(3, dtype=torch.float32)#x=tensor([0., 1., 2.])\n",
    "A,x,A.shape, x.shape, torch.mv(A, x), A@x \n",
    "\n",
    "#Ax=(tensor([[0., 1., 2.],[3., 4., 5.]])*tensor([0., 1., 2.])=tensor([ 5., 14.])=A@x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.10 Matrix-Matrix Multiplication\n",
    "\n",
    "If you have gotten the hang of dot products and matrix-vector products, then matrix-matrix multiplication should be straightforward.\n",
    "Say that we have two matrices $A \\in \\mathbb{R}^{n \\times k}$ and $B \\in \\mathbb{R}^{k \\times m}$\n",
    "\n",
    "2.3.10 Multiplication matrice-matrice\n",
    "\n",
    "Si vous maîtrisez les produits scalaires et les produits matrice-vecteur, la multiplication matrice-matrice devrait être simple.\n",
    "Disons que nous avons deux matrices $A \\in \\mathbb{R}^{n \\times k}$ et $B \\in \\mathbb{R}^{k \\times m}$\n",
    "\n",
    "$$\n",
    "A=\\begin{pmatrix}\n",
    "a_{11} &~ a_{12} & . . . &a_{1k} \\\\\n",
    "a_{21} & a_{22} & . . . &a_{2k} \\\\\n",
    ". & . & . & . \\\\                       \n",
    ". & . & . & . \\\\\n",
    ". & . & . & . \\\\\n",
    " a_{n1} &  a_{n2} & . . . &a_{nk}\n",
    "\\end{pmatrix} ,\\quad B=\\begin{pmatrix}\n",
    "b_{11} &~ b_{12} & . . . &b_{1m} \\\\\n",
    "b_{21} & b_{22} & . . . &b_{2m} \\\\\n",
    ". & . & . & . \\\\                       \n",
    ". & . & . & . \\\\\n",
    ". & . & . & . \\\\\n",
    " b_{k1} &  a_{k2} & . . . &a_{km}\n",
    "\\end{pmatrix} ,\\quad  \\quad (2.3.7)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $a_{i}^{T} \\in \\mathbb{R}^{k}$ denote the row vector representing the $i^{th}$ row of the matrix A and let $b_{j} \\in \\mathbb{R}^{k}$ denote the column vector from the $j^{th}$ column of the matrix B:\n",
    "\n",
    "Soit $a_{i}^{T} \\in \\mathbb{R}^{k}$ le vecteur ligne représentant la $i^{ième}$ ligne de la matrice A et soit $b_{j} \\in \\mathbb{R}^{k}$ désigne le vecteur colonne de la colonne $j^{th}$ de la matrice B :\n",
    "$$\n",
    "A=\\left( \\begin{array}{c}\n",
    "a_{1}^{T} \\\\\n",
    "a_{2}^{T} \\\\\n",
    "\\vdots \\\\\n",
    "a_{i}^{T} \\\\\n",
    "\\vdots \\\\\n",
    "a_{n}^{T}\n",
    "\\end{array} \\right) , \\quad B=  \\left( \\begin{array}{c}\n",
    "b_{1}& b_{2} & . & . & . & b_{m}\n",
    "\\end{array} \\right)\\quad \\quad (2.3.8)\n",
    "$$\n",
    "To form the matrix product $C \\in \\mathbb{R}^{n \\times m}$ , we simply compute each element $c_{ij}$ as the dot product between the $i^{th}$ row of A and the $j^{th}$ column of B, i.e., $a_{i}^{T}b_{j}$:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "C=AB=\\left( \\begin{array}{c}\n",
    "a_{1}^{T} \\\\\n",
    "a_{2}^{T} \\\\\n",
    "\\vdots \\\\\n",
    "a_{i}^{T} \\\\\n",
    "\\vdots \\\\\n",
    "a_{n}^{T}\n",
    "\\end{array} \\right)  \\left( \\begin{array}{c}\n",
    "b_{1}& b_{2} & . & . & . & b_{m}\n",
    "\\end{array} \\right) = \\left( \\begin{array}{c}\n",
    "a_{1}^{T}b_{1}&a_{1}^{T}b_{2} &.&.&.&a_{1}^{T}b_{m}\\\\\n",
    "a_{2}^{T}b_{1}&a_{2}^{T}b_{2} &.&.&.&a_{2}^{T}b_{m} \\\\\n",
    "\\vdots \\\\\n",
    "a_{i}^{T}b_{1}&a_{i}^{T}b_{2} &.&.&.&a_{i}^{T}b_{m} \\\\\n",
    "\\vdots \\\\\n",
    "a_{n}^{T}b_{1}&a_{n}^{T}b_{2} &.&.&.&a_{n}^{T}b_{m}\n",
    "\\end{array} \\right) ,\\quad \\quad (2.3.9)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of the matrix-matrix multiplication AB as performing m matrix-vector products or m × n dot products and stitching the results together to form an n × m matrix. In the following snippet, we perform matrix multiplication on A and B. Here, A is a matrix with 2\n",
    "rows and 3 columns, and B is a matrix with 3 rows and 4 columns. After multiplication, we obtain a matrix with 2 rows and 4 columns.\n",
    "\n",
    "Nous pouvons penser que la multiplication matrice-matrice AB consiste à effectuer m produits matrice-vecteur ou m × n produits scalaires et à assembler les résultats pour former une matrice n × m. Dans l'extrait suivant, nous effectuons une multiplication matricielle sur A et B. Ici, A est une matrice avec 2 lignes et 3 colonnes, et B est une matrice de 3 lignes et 4 colonnes. Après multiplication, on obtient une matrice à 2 lignes et 4 colonnes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 2.],\n",
       "         [3., 4., 5.]]),\n",
       " tensor([[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]),\n",
       " tensor([[ 3.,  3.,  3.,  3.],\n",
       "         [12., 12., 12., 12.]]),\n",
       " tensor([[ 3.,  3.,  3.,  3.],\n",
       "         [12., 12., 12., 12.]]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(6, dtype=torch.float32).reshape(2, 3)#A=(tensor([[0., 1., 2.],[3., 4., 5.]]),\n",
    "B = torch.ones(3, 4)#B=(tensor([[1., 1., 1, 1.],[1., 1., 1, 1.],[1., 1., 1, 1.]]),\n",
    "A,B,torch.mm(A, B), A@B # AB=A@B=(tensor([[3., 3., 3, 3.],[12., 12., 12., 12.]]),\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term matrix-matrix multiplication is often simplified to matrix multiplication, and should not be confused with the Hadamard product.\n",
    "\n",
    "Le terme multiplication matrice-matrice est souvent simplifié en multiplication matricielle et ne doit pas être confondu avec le produit Hadamard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.11 Norms\n",
    "\n",
    "Some of the most useful operators in linear algebra are norms. Informally, the norm of a vector tells us how big it is. For instance, the ℓ2 norm measures the (Euclidean) length of a vector. Here, we are employing a notion of size that concerns the magnitude of a vector’s\n",
    "components (not its dimensionality). A norm is a function ∥ · ∥ that maps a vector to a scalar and satisfies the following three\n",
    "properties:\n",
    "1. Given any vector x, if we scale (all elements of) the vector by a scalar α ∈ R, its norm scales accordingly: ∥αx∥ = |α|∥x∥. (2.3.10)\n",
    "2. For any vectors x and y: norms satisfy the triangle inequality: ∥x + y∥ ≤ ∥x∥ + ∥y∥. (2.3.11)\n",
    "3. The norm of a vector is nonnegative and it only vanishes if the vector is zero: ∥x∥ > 0 for all $x\\neq 0$. (2.3.12)\n",
    "\n",
    "2.3.11 Normes\n",
    "\n",
    "Certains des opérateurs les plus utiles en algèbre linéaire sont les normes. De manière informelle, la norme d'un vecteur nous indique sa taille. Par exemple, la norme ℓ2 mesure la longueur (euclidienne) d'un vecteur. Ici, nous employons une notion de taille qui concerne la grandeur de la\n",
    "composants (pas sa dimensionnalité). Une norme est une fonction ∥ · ∥ qui fait correspondre un vecteur à un scalaire et satisfait les trois conditions suivantes\n",
    "propriétés:\n",
    "\n",
    "1. Étant donné tout vecteur x, si nous mettons à l'échelle (tous les éléments de) le vecteur par un scalaire α ∈ R, sa norme est mise à l'échelle en conséquence : ∥αx∥ = |α|∥x∥. (2.3.10)\n",
    "2. Pour tous les vecteurs x et y : les normes satisfont l'inégalité triangulaire : ∥x + y∥ ≤ ∥x∥ + ∥y∥. (2.3.11)\n",
    "3. La norme d'un vecteur est non négative et elle ne s'annule que si le vecteur est nul: ∥x∥ > 0 for all $x\\neq 0$. (2.3.12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many functions are valid norms and different norms encode different notions of size. The Euclidean norm that we all learned in elementary school geometry when calculating the hypotenuse of right triangle is the square root of the sum of squares of a vector’s elements.\n",
    "Formally, this is called the $ ℓ_{2}$ norm and expressed as  $$\\|x\\|=\\sqrt{ \\sum _{i=1}^{n} {x_{i}^{2}}} \\quad \\quad (2.3.13)$$\n",
    "The method norm calculates the $ ℓ_{2}$ norm.\n",
    "\n",
    "De nombreuses fonctions sont des normes valides et différentes normes codent différentes notions de taille. La norme euclidienne que nous avons tous apprise en géométrie à l'école élémentaire lors du calcul de l'hypoténuse d'un triangle rectangle est la racine carrée de la somme des carrés des éléments d'un vecteur.\n",
    "Formellement, cela s'appelle la norme $ ℓ_{2}$ et s'exprime sous la forme $$\\|x\\|=\\sqrt{ \\sum _{i=1}^{n} {x_{i}^{2}}} \\quad \\quad (2.3.13)$$\n",
    "La méthode norm calcule la norme $ ℓ_{2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = torch.tensor([3.0, -4.0])\n",
    "torch.norm(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $ ℓ_{1}$ norm is also popular and the associated metric is called the Manhattan distance. By definition, the $ ℓ_{1}$ norm sums the absolute values of a vector’s elements: $$\\|x\\|_{1}=\\sum _{i=1}^{n} {|x_{i}|} \\quad \\quad (2.3.14)$$\n",
    "Compared to the $ ℓ_{2}$ norm, it is less sensitive to outliers. To compute the $ ℓ_{1}$ norm, we compose\n",
    "the absolute value with the sum operation.\n",
    "\n",
    "La norme $ ℓ_{1}$ est également populaire et la métrique associée s'appelle la distance de Manhattan. Par définition, la norme $ ℓ_{1}$ additionne les valeurs absolues des éléments d'un vecteur : $$\\|x\\|_{1}=\\sum _{i=1}^{n} {|x_{i} |} \\quad \\quad (2.3.14)$$\n",
    "Par rapport à la norme $ ℓ_{2}$, elle est moins sensible aux valeurs aberrantes. Pour calculer la norme $ ℓ_{1}$, on compose\n",
    "la valeur absolue avec l'opération de somme.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3., -4.]), tensor(7.))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = torch.tensor([3.0, -4.0])\n",
    "u,torch.abs(u).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the $ ℓ_{2}$ and $ ℓ_{1}$ norms are special cases of the more general $ ℓ_{p}$ norms:\n",
    "$$\\|x\\|_{p}=(\\sum _{i=1}^{n} {|x_{i} |^ {p}})^{1/p} \\quad \\quad (2.3.15)$$\n",
    "In the case of matrices, matters are more complicated. After all, matrices can be viewed both as collections of individual entries and as objects that operate on vectors and transform them into other vectors. For instance, we can ask by how much longer the matrix-vector product\n",
    "Xv could be relative to v. This line of thought leads to a norm called the spectral norm. For now, we introduce the Frobenius norm, which is much easier to compute and defined as the square root of the sum of the squares of a matrix’s elements:\n",
    "\n",
    "Les normes $ ℓ_{2}$ et $ ℓ_{1}$ sont des cas particuliers des normes plus générales $ ℓ_{p}$ :\n",
    "$$\\|x\\|_{p}=(\\sum _{i=1}^{n} {|x_{i} |^ {p}})^{1/p} \\quad \\quad (2.3 .15)$$\n",
    "Dans le cas des matrices, les choses sont plus compliquées. Après tout, les matrices peuvent être considérées à la fois comme des collections d'entrées individuelles et comme des objets qui opèrent sur des vecteurs et les transforment en d'autres vecteurs. Par exemple, nous pouvons demander de combien de temps le produit matrice-vecteur\n",
    "Xv pourrait être relatif à v. Cette ligne de pensée conduit à une norme appelée la norme spectrale. Pour l'instant, nous introduisons la norme de Frobenius, beaucoup plus facile à calculer et définie comme la racine carrée de la somme des carrés des éléments d'une matrice :\n",
    "$$\\|x\\|_{F}=\\sqrt{ \\sum _{i=1}^{m} \\sum _{j=1}^{n}{x_{ij}^{2}}} \\quad \\quad (2.3.16)$$\n",
    "The Frobenius norm behaves as if it were an $ ℓ_{2}$ norm of a matrix-shaped vector. Invoking the following function will calculate the Frobenius norm of a matrix.\n",
    "\n",
    "La norme de Frobenius se comporte comme s'il s'agissait d'une norme $ ℓ_{2}$ d'un vecteur en forme de matrice. L'appel de la fonction suivante calculera la norme de Frobenius d'une matrice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1.]]),\n",
       " tensor(6.))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.ones((4, 9)),torch.norm(torch.ones((4, 9)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we do not want to get too far ahead of ourselves, we can plant some intuition already about why these concepts are useful. In deep learning, we are often trying to solve optimization problems: maximize the probability assigned to observed data; maximize the revenue\n",
    "associated with a recommender model; minimize the distance between predictions and the ground-truth observations; minimize the distance between representations of photos of the same person while maximizing the distance between representations of photos of different\n",
    "people. These distances, which constitute the objectives of deep learning algorithms, are often expressed as norms.\n",
    "\n",
    "Bien que nous ne voulions pas trop nous dépasser, nous pouvons déjà avoir une idée de l'utilité de ces concepts. En apprentissage profond, on cherche souvent à résoudre des problèmes d'optimisation : maximiser la probabilité attribuée aux données observées ; maximiser les revenus\n",
    "associé à un modèle de recommandation ; minimiser la distance entre les prédictions et les observations de la vérité sur le terrain ; minimiser la distance entre les représentations de photos de la même personne tout en maximisant la distance entre les représentations de photos de différentes personnes. Ces distances, qui constituent les objectifs des algorithmes d'apprentissage profond, sont souvent exprimées sous forme de normes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.12 Discussion\n",
    "\n",
    "In this section, we reviewed all the linear algebra that you will need to understand a remarkable chunk of modern deep learning. There is a lot more to linear algebra and much of it is useful for machine learning. For example, matrices can be decomposed into factors, and these\n",
    "decompositions can reveal low-dimensional structure in real-world datasets. There are entire\n",
    "subfields of machine learning that focus on using matrix decompositions and their generalizations to high-order tensors to discover structure in datasets and solve prediction problems. But this book focuses on deep learning. And we believe you will be more inclined to learn\n",
    "more mathematics once you have gotten your hands dirty applying machine learning to real datasets. So while we reserve the right to introduce more mathematics later on, we wrap up this section here.\n",
    "If you are eager to learn more linear algebra, there are many excellent books and online resources. For a more advanced crash course, consider checking out Kolter (2008), Petersen et al. (2008), Strang (1993).\n",
    "\n",
    "\n",
    "2.3.12 Discussion\n",
    "\n",
    "Dans cette section, nous avons passé en revue toute l'algèbre linéaire dont vous aurez besoin pour comprendre une partie remarquable de l'apprentissage en profondeur moderne. Il y a beaucoup plus dans l'algèbre linéaire et une grande partie est utile pour l'apprentissage automatique. Par exemple, les matrices peuvent être décomposées en facteurs, et ceux-ci les décompositions peuvent révéler une structure de faible dimension dans des ensembles de données du monde réel. Il y a des entiers sous-domaines de l'apprentissage automatique qui se concentrent sur l'utilisation des décompositions matricielles et leurs généralisations aux tenseurs d'ordre élevé pour découvrir la structure des ensembles de données et résoudre les problèmes de prédiction. Mais ce livre se concentre sur l'apprentissage en profondeur. Et nous pensons que vous serez plus enclin à apprendre plus de mathématiques une fois que vous avez mis la main à la pâte en appliquant l'apprentissage automatique à de vrais ensembles de données. Ainsi, bien que nous nous réservions le droit d'introduire plus de mathématiques plus tard, nous terminons cette section ici. Si vous souhaitez en savoir plus sur l'algèbre linéaire, il existe de nombreux excellents livres et ressources en ligne. Pour un cours accéléré plus avancé, pensez à consulter Kolter (2008), Petersen et al. (2008), Strang (1993)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nom_environnement",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
